{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "current_file_path = Path(os.path.abspath('')).resolve()\n",
    "# print(current_file_path.parent)\n",
    "sys.path.insert(0, str(current_file_path.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from src.utils.misc import read_config\n",
    "from src.slot_attention import UOD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers.utils import make_image_grid\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import torchvision.transforms as T\n",
    "import json\n",
    "\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_scripts.train_editor import SlotDataset\n",
    "from train_scripts.train_editor import TrainingConfig, SlotEditor, unnormalize, image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_residual = True\n",
    "val_pickle_file:str = \"/home/cse/btech/cs1210561/scratch/SA/output/clevr_run_res112_try_cont/val_data_latest_wo_112_res_cont.pickle\"\n",
    "val_image_root:str = '/home/scai/phd/aiz228170/scratch/Datasets/CIM-NLI/combined/valid' \n",
    "val_json_file:str = '/home/cse/btech/cs1210561/scratch/combined/valid/CLEVR_questions.json'\n",
    "use_saved_t5 = False\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "checkpoint_to_vis = \"/home/cse/btech/cs1210561/scratch/SA/editor_runs/cont_512_4_8_4_300_4096_64_64_1_4e-4_1000_on_train_l2_clip_1_cosine_lr_200ep/checkpoints/epoch_72_step_121105.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_with_caption(image_array, caption, save_path):\n",
    "    \"\"\"Saves an image with a caption overlaid at the bottom.\"\"\"\n",
    "    image = Image.fromarray(image_array)\n",
    "    \n",
    "    img_width, img_height = image.size\n",
    "    caption_height = 40  \n",
    "    new_image = Image.new(\"RGB\", (img_width, img_height + caption_height), (255, 255, 255))\n",
    "    new_image.paste(image, (0, 0))\n",
    "\n",
    "    draw = ImageDraw.Draw(new_image)\n",
    "    font_size = 20\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    bbox = draw.textbbox((0, 0), caption, font=font)\n",
    "    text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "    \n",
    "    text_x = (img_width - text_width) // 2\n",
    "    text_y = img_height + (caption_height - text_height) // 2\n",
    "\n",
    "    draw.text((text_x, text_y), caption, fill=\"black\", font=font)\n",
    "    \n",
    "    new_image.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.inference_mode()\n",
    "def log_validation(model, vis_model, val_batch, global_step, device, samples_2_show=4, save_dir = \"saved_images_on_val_all\",text_encoder=None, tokenizer=None):\n",
    "    # print(\"Logging Visualization\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model = model.eval()\n",
    "    vis_model = vis_model.eval()\n",
    "\n",
    "    input_slots = val_batch['input_slots'][:samples_2_show].to(device)\n",
    "    target_slots = val_batch['output_slots'][:samples_2_show].to(device)\n",
    "\n",
    "    if use_saved_t5 and (text_encoder is None and tokenizer is None):\n",
    "        y = val_batch['text_emb'][:samples_2_show].to(device).squeeze(1)\n",
    "        y_mask = val_batch['emb_mask'][:samples_2_show].to(device).squeeze(1)\n",
    "    else:\n",
    "        max_length = config.model_max_length\n",
    "        txt_tokens = tokenizer(val_batch['edit_prompt'][:samples_2_show], max_length=max_length, padding='max_length', truncation=True,\n",
    "        return_tensors='pt').to(device)\n",
    "        \n",
    "        y = text_encoder(txt_tokens.input_ids, attention_mask=txt_tokens.attention_mask)[0][:,None]\n",
    "        y_mask = y_mask = txt_tokens.attention_mask[:, None, None]\n",
    "\n",
    "    src_images = unnormalize(val_batch['image_name'][:samples_2_show])\n",
    "    tgt_images = unnormalize(val_batch['out_image_name'][:samples_2_show])\n",
    "\n",
    "    img_stack = [np.hstack((np.asarray(image_from_tensor(og)),np.asarray(image_from_tensor(tgt)))) for og,tgt in zip(src_images, tgt_images)]\n",
    "\n",
    "\n",
    "    output_slots = model(input_slots, y, y_mask)\n",
    "    if predict_residual:\n",
    "        output_slots  = input_slots + output_slots\n",
    "    # print(output_slots.shape)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    dec_output = vis_model.slot_to_img(output_slots)\n",
    "    dec_img_gt_slots =  vis_model.slot_to_img(target_slots)\n",
    "\n",
    "    gen_tensor = unnormalize(dec_output['generated'])\n",
    "    gt_gen_tensor = unnormalize(dec_img_gt_slots['generated'])\n",
    "    \n",
    "    bs = output_slots.shape[0]\n",
    "    num_slots = output_slots.shape[1]\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    import wandb\n",
    "    # formatted_images = []\n",
    "    for idx in range(bs):\n",
    "        istack = img_stack[idx]\n",
    "        gt_reconstructed_image = image_from_tensor(gt_gen_tensor[idx])\n",
    "        image_reconstructed = image_from_tensor(gen_tensor[idx])\n",
    "        slot_masks = np.hstack([Image.fromarray((dec_output['masks'][idx][k].squeeze().cpu().detach().numpy()*255).astype(np.uint8)).convert('RGB') for k in range(num_slots)])\n",
    "        final_img = np.hstack((istack, gt_reconstructed_image, image_reconstructed, slot_masks))\n",
    "        # image = wandb.Image(final_img, caption=val_batch['edit_prompt'][idx])\n",
    "        # formatted_images.append(image)\n",
    "        image_path = os.path.join(save_dir, f\"validation_{global_step}_{idx}.png\")\n",
    "        save_image_with_caption(final_img, val_batch['edit_prompt'][idx], image_path)\n",
    "    \n",
    "\n",
    "    \n",
    "    # del vis_model\n",
    "    # return image_logs\n",
    "    return ((gt_gen_tensor-gen_tensor)**2).mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a859408c674f68b168cfe998bf42ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = TrainingConfig()\n",
    "model = SlotEditor(\n",
    "            hidden_size=config.hidden_size,\n",
    "            depth=config.depth,\n",
    "            num_heads=config.num_heads,\n",
    "            mlp_ratio=config.mlp_ratio,\n",
    "            model_max_length=config.model_max_length,\n",
    "            caption_channels=config.caption_channels,\n",
    "            slot_dim=config.slot_dim\n",
    "        )\n",
    "\n",
    "sa_config = read_config(config.config_file)\n",
    "sa_model = UOD(sa_config)\n",
    "\n",
    "ckpt = torch.load(config.checkpoint_path)\n",
    "sa_model.load_state_dict(ckpt['state_dict'])\n",
    "for param in sa_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "tokenizer = text_encoder = None\n",
    "if not use_saved_t5:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(config.pipeline_load_from, subfolder=\"tokenizer\")\n",
    "    text_encoder = T5EncoderModel.from_pretrained(\n",
    "        config.pipeline_load_from, subfolder=\"text_encoder\", torch_dtype=torch.float16).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ckpt_file = checkpoint_to_vis\n",
    "checkpoint = torch.load(ckpt_file, map_location=\"cpu\")\n",
    "\n",
    "state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "\n",
    "missing, unexpect = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "sa_model = sa_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 , MSE : 0.0\n",
      "Step:  10 , MSE : tensor(0.0062, device='cuda:0')\n",
      "Step:  20 , MSE : tensor(0.0157, device='cuda:0')\n",
      "Step:  30 , MSE : tensor(0.0242, device='cuda:0')\n",
      "Step:  40 , MSE : tensor(0.0321, device='cuda:0')\n",
      "Step:  50 , MSE : tensor(0.0398, device='cuda:0')\n",
      "Step:  60 , MSE : tensor(0.0458, device='cuda:0')\n",
      "Step:  70 , MSE : tensor(0.0531, device='cuda:0')\n",
      "Step:  80 , MSE : tensor(0.0608, device='cuda:0')\n",
      "Step:  90 , MSE : tensor(0.0774, device='cuda:0')\n",
      "Step:  100 , MSE : tensor(0.0957, device='cuda:0')\n",
      "Step:  110 , MSE : tensor(0.1147, device='cuda:0')\n",
      "Step:  120 , MSE : tensor(0.1373, device='cuda:0')\n",
      "Step:  130 , MSE : tensor(0.1513, device='cuda:0')\n",
      "Step:  140 , MSE : tensor(0.1678, device='cuda:0')\n",
      "Step:  150 , MSE : tensor(0.1877, device='cuda:0')\n",
      "Step:  160 , MSE : tensor(0.1948, device='cuda:0')\n",
      "Step:  170 , MSE : tensor(0.2012, device='cuda:0')\n",
      "Step:  180 , MSE : tensor(0.2093, device='cuda:0')\n",
      "Step:  190 , MSE : tensor(0.2166, device='cuda:0')\n",
      "Step:  200 , MSE : tensor(0.2256, device='cuda:0')\n",
      "Step:  210 , MSE : tensor(0.2325, device='cuda:0')\n",
      "Step:  220 , MSE : tensor(0.2448, device='cuda:0')\n",
      "Avg MSE: tensor(0.0011, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "val_ds = SlotDataset(val_pickle_file, val_image_root,val_json_file, hop_type = None)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=1)\n",
    "\n",
    "mse = 0.0\n",
    "steps = 0\n",
    "for step, batch in enumerate(val_dl):\n",
    "    if step%10 ==0:\n",
    "        print(\"Step: \",step, \", MSE :\",mse)\n",
    "    mse+=log_validation(model, sa_model, batch, step, device=device, samples_2_show=2,save_dir = \"saved_images_on_val_all\", text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "    steps +=1\n",
    "\n",
    "print(\"Avg MSE:\",mse/steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 , MSE : 0.0\n",
      "Step:  10 , MSE : tensor(0.0092, device='cuda:0')\n",
      "Step:  20 , MSE : tensor(0.0174, device='cuda:0')\n",
      "Step:  30 , MSE : tensor(0.0250, device='cuda:0')\n",
      "Step:  0 , MSE : tensor(0.0305, device='cuda:0')\n",
      "Step:  10 , MSE : tensor(0.0403, device='cuda:0')\n",
      "Step:  20 , MSE : tensor(0.0513, device='cuda:0')\n",
      "Step:  30 , MSE : tensor(0.0674, device='cuda:0')\n",
      "Step:  40 , MSE : tensor(0.0820, device='cuda:0')\n",
      "Step:  50 , MSE : tensor(0.0963, device='cuda:0')\n",
      "Step:  60 , MSE : tensor(0.1073, device='cuda:0')\n",
      "Avg MSE for 0-1 hop data: tensor(0.0011, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "val_ds = SlotDataset(val_pickle_file, val_image_root,val_json_file, hop_type = 0)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=1)\n",
    "mse = 0.0\n",
    "steps = 0\n",
    "for step, batch in enumerate(val_dl):\n",
    "    if step%10 ==0:\n",
    "        print(\"Step: \",step, \", MSE :\",mse)\n",
    "    \n",
    "    #Wrong name for save dir(it should be train)\n",
    "    mse+=log_validation(model, sa_model, batch, steps, device=device, samples_2_show=2,save_dir = \"saved_images_on_val_01hop\", text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "    steps +=1\n",
    "\n",
    "val_ds = SlotDataset(val_pickle_file, val_image_root,val_json_file, hop_type = 1)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=1)\n",
    "for step, batch in enumerate(val_dl):\n",
    "    if step%10 ==0:\n",
    "        print(\"Step: \",step, \", MSE :\",mse)\n",
    "    mse+=log_validation(model, sa_model, batch, steps, device=device, samples_2_show=2,save_dir = \"saved_images_on_val_01hops\", text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "    steps +=1\n",
    "\n",
    "print(\"Avg MSE for 0-1 hop data:\",mse/steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_to_vis = \"/home/cse/btech/cs1210561/scratch/SA/editor_runs/cont_512_4_8_4_300_4096_64_64_1_4e-4_1000_on_val_l2_clip_1_cosine_lr_1000ep/checkpoints/epoch_1000_step_452001.pth\"\n",
    "\n",
    "\n",
    "ckpt_file = checkpoint_to_vis\n",
    "checkpoint = torch.load(ckpt_file, map_location=\"cpu\")\n",
    "\n",
    "state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "\n",
    "missing, unexpect = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "sa_model = sa_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 , MSE : 0.0\n",
      "Step:  10 , MSE : tensor(0.0013, device='cuda:0')\n",
      "Step:  20 , MSE : tensor(0.0055, device='cuda:0')\n",
      "Step:  30 , MSE : tensor(0.0076, device='cuda:0')\n",
      "Step:  40 , MSE : tensor(0.0096, device='cuda:0')\n",
      "Step:  50 , MSE : tensor(0.0126, device='cuda:0')\n",
      "Step:  60 , MSE : tensor(0.0139, device='cuda:0')\n",
      "Step:  70 , MSE : tensor(0.0149, device='cuda:0')\n",
      "Step:  80 , MSE : tensor(0.0168, device='cuda:0')\n",
      "Step:  90 , MSE : tensor(0.0189, device='cuda:0')\n",
      "Step:  100 , MSE : tensor(0.0220, device='cuda:0')\n",
      "Step:  110 , MSE : tensor(0.0247, device='cuda:0')\n",
      "Step:  120 , MSE : tensor(0.0267, device='cuda:0')\n",
      "Step:  130 , MSE : tensor(0.0291, device='cuda:0')\n",
      "Step:  140 , MSE : tensor(0.0311, device='cuda:0')\n",
      "Step:  150 , MSE : tensor(0.0354, device='cuda:0')\n",
      "Step:  160 , MSE : tensor(0.0361, device='cuda:0')\n",
      "Step:  170 , MSE : tensor(0.0369, device='cuda:0')\n",
      "Step:  180 , MSE : tensor(0.0378, device='cuda:0')\n",
      "Step:  190 , MSE : tensor(0.0384, device='cuda:0')\n",
      "Step:  200 , MSE : tensor(0.0390, device='cuda:0')\n",
      "Step:  210 , MSE : tensor(0.0398, device='cuda:0')\n",
      "Step:  220 , MSE : tensor(0.0407, device='cuda:0')\n",
      "Avg MSE: tensor(0.0002, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "val_ds = SlotDataset(val_pickle_file, val_image_root,val_json_file, hop_type = None)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=1)\n",
    "\n",
    "mse = 0.0\n",
    "steps = 0\n",
    "for step, batch in enumerate(val_dl):\n",
    "    if step%10 ==0:\n",
    "        print(\"Step: \",step, \", MSE :\",mse)\n",
    "    mse+=log_validation(model, sa_model, batch, step, device=device, samples_2_show=2,save_dir = \"saved_images_on_train_all\", text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "    steps +=1\n",
    "\n",
    "print(\"Avg MSE:\",mse/steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 , MSE : 0.0\n",
      "Step:  10 , MSE : tensor(0.0014, device='cuda:0')\n",
      "Step:  20 , MSE : tensor(0.0029, device='cuda:0')\n",
      "Step:  30 , MSE : tensor(0.0036, device='cuda:0')\n",
      "Step:  0 , MSE : tensor(0.0042, device='cuda:0')\n",
      "Step:  10 , MSE : tensor(0.0068, device='cuda:0')\n",
      "Step:  20 , MSE : tensor(0.0096, device='cuda:0')\n",
      "Step:  30 , MSE : tensor(0.0130, device='cuda:0')\n",
      "Step:  40 , MSE : tensor(0.0158, device='cuda:0')\n",
      "Step:  50 , MSE : tensor(0.0176, device='cuda:0')\n",
      "Step:  60 , MSE : tensor(0.0181, device='cuda:0')\n",
      "Avg MSE for 0-1 hop data: tensor(0.0002, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "val_ds = SlotDataset(val_pickle_file, val_image_root,val_json_file, hop_type = 0)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=1)\n",
    "mse = 0.0\n",
    "steps = 0\n",
    "for step, batch in enumerate(val_dl):\n",
    "    if step%10 ==0:\n",
    "        print(\"Step: \",step, \", MSE :\",mse)\n",
    "    mse+=log_validation(model, sa_model, batch, steps, device=device, samples_2_show=2,save_dir = \"saved_images_on_train_01hop\", text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "    steps +=1\n",
    "\n",
    "val_ds = SlotDataset(val_pickle_file, val_image_root,val_json_file, hop_type = 1)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=1)\n",
    "for step, batch in enumerate(val_dl):\n",
    "    if step%10 ==0:\n",
    "        print(\"Step: \",step, \", MSE :\",mse)\n",
    "    mse+=log_validation(model, sa_model, batch, steps, device=device, samples_2_show=2,save_dir = \"saved_images_on_train_01hops\", text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "    steps +=1\n",
    "\n",
    "print(\"Avg MSE for 0-1 hop data:\",mse/steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pixart)",
   "language": "python",
   "name": "pixart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
